{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86aa92e4",
   "metadata": {},
   "source": [
    "# MLE challenge - Features engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "279a1298",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430546a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import datediff,col, current_date\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de8e4470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/18 20:13:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Kueski').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e94bdb99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c6817f8168be:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Kueski</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7feb101576a0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c933b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header', 'true').csv('dataset_credit_risk.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6dfe810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------+----------+------------------+----------+--------------+------+\n",
      "|loan_id|     id|flag_own_car|  birthday|       loan_amount| loan_date|job_start_date|status|\n",
      "+-------+-------+------------+----------+------------------+----------+--------------+------+\n",
      "| 208089|5044500|           N|1955-08-04|  133.714973572794|2019-01-01|    3021-09-18|     0|\n",
      "| 112797|5026631|           N|1972-03-30|158.80055787554005|2019-01-01|    1997-06-05|     0|\n",
      "| 162434|5036645|           Y|1987-03-24|203.60848690335118|2019-01-01|    2015-02-22|     0|\n",
      "| 144343|5033584|           N|1973-03-15|113.20496431707618|2019-01-01|    2009-06-29|     0|\n",
      "| 409695|5085755|           Y|1989-10-15| 109.3762599318495|2019-01-01|    2019-07-03|     0|\n",
      "+-------+-------+------------+----------+------------------+----------+--------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df_pyspark.select('loan_id','id', 'flag_own_car', 'birthday', 'loan_amount', 'loan_date', 'job_start_date','status')\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "446b25df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_id: integer (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- flag_own_car: string (nullable = true)\n",
      " |-- birthday: string (nullable = true)\n",
      " |-- loan_amount: float (nullable = true)\n",
      " |-- loan_date: date (nullable = true)\n",
      " |-- job_start_date: string (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn('loan_date', df.loan_date.cast('date'))\n",
    "df = df.withColumn('loan_amount', df.loan_amount.cast('float'))\n",
    "df = df.withColumn('loan_id', df.loan_amount.cast('int'))\n",
    "df = df.orderBy('id','loan_date')\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce918ab8",
   "metadata": {},
   "source": [
    "#### Feature avg_amount_loans_previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65852a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- avg_amount_loans_previous: float (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/pandas/functions.py:389: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@F.pandas_udf('float', F.PandasUDFType.GROUPED_AGG)  \n",
    "def mean_udf(s):\n",
    "    return s[:-1].mean()\n",
    "\n",
    "df2 = df.groupBy('id').agg(mean_udf('loan_amount'))\n",
    "df2 = df2.withColumnRenamed('mean_udf(loan_amount)', 'avg_amount_loans_previous')\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02082c",
   "metadata": {},
   "source": [
    "#### Feature nb_previous_loans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b5ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = (df\n",
    ".groupBy(F.col('id'))\n",
    ".agg(F.count('id').alias('nb_previous_loans')))\n",
    "df3=df3.withColumn(\"nb_previous_loans\", df3.nb_previous_loans - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a12326",
   "metadata": {},
   "source": [
    "#### The last update of a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77650d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- loan_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_last_loan = df.groupBy('id').max('loan_id') \n",
    "df_last_loan = df_last_loan.withColumnRenamed('max(loan_id)', 'loan_id')\n",
    "df_last_loan.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e272fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df.join(df_last_loan, on=['loan_id', 'id'], how='inner')\n",
    "df_f = df_f.join(df2, on=['id'], how='inner')\n",
    "df_f = df_f.join(df3, on=['id'], how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e743fb",
   "metadata": {},
   "source": [
    "#### Feature age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed53be37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df_f.withColumn(\"age\", datediff(current_date(),col(\"birthday\"))/365.25)\n",
    "df_f = df_f.withColumn('age', df_f.age.cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b358b9e1",
   "metadata": {},
   "source": [
    "#### Feature years_on_the_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c655109b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f = df_f.withColumn(\"years_on_the_job\", datediff(current_date(),col(\"job_start_date\"))/365.25)\n",
    "df_f = df_f.withColumn('years_on_the_job', df_f.years_on_the_job.cast('int'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e058043b",
   "metadata": {},
   "source": [
    "#### Feature flag_own_car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc174333",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol=\"flag_own_car\", outputCol=\"flag_own_car_2\")\n",
    "indexed = indexer.fit(df_f).transform(df_f)\n",
    "\n",
    "df_f = indexed.withColumn('flag_own_car_2', indexed.flag_own_car_2.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8980c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_f.select( 'id',\n",
    "                        'avg_amount_loans_previous',\n",
    "                        'nb_previous_loans',\n",
    "                        'flag_own_car_2',\n",
    "                        'age',\n",
    "                        'years_on_the_job',\n",
    "                        'status')\n",
    "df_train = df_train.withColumnRenamed('flag_own_car_2', 'flag_own_car')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06f90526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:==================================================>  (190 + 10) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------------+-----------------+------------+---+----------------+------+\n",
      "|     id|avg_amount_loans_previous|nb_previous_loans|flag_own_car|age|years_on_the_job|status|\n",
      "+-------+-------------------------+-----------------+------------+---+----------------+------+\n",
      "|5132977|                131.33649|               13|           0| 40|               3|     0|\n",
      "|5126737|                140.70323|                3|           0| 45|               3|     0|\n",
      "|5126852|                138.86935|               20|           1| 34|              10|     0|\n",
      "|5041229|                131.68132|                6|           0| 46|              28|     0|\n",
      "|5041228|                120.65634|                8|           0| 46|              28|     0|\n",
      "|5089760|                139.43625|               11|           1| 22|               2|     0|\n",
      "|5067022|                121.39591|               23|           1| 47|               3|     0|\n",
      "|5095626|                127.54001|               23|           0| 28|               1|     0|\n",
      "|5058355|                112.96307|                6|           0| 27|               5|     0|\n",
      "|5033589|                137.83981|                5|           0| 43|               6|     0|\n",
      "|5067026|                162.61635|               17|           0| 37|               4|     0|\n",
      "|5143236|                137.73747|                3|           1| 27|               6|     0|\n",
      "|5053237|                130.42584|               14|           1| 39|               0|     0|\n",
      "|5089392|               127.668564|                9|           1| 40|               7|     0|\n",
      "|5022238|                 128.4877|               36|           1| 39|              17|     0|\n",
      "|5125747|                121.82201|               17|           0| 56|            -999|     0|\n",
      "|5091449|               121.666534|               18|           0| 42|               2|     0|\n",
      "|5090179|                126.50318|                7|           1| 50|               7|     0|\n",
      "|5079197|                 122.3152|                7|           0| 53|               8|     0|\n",
      "|5048592|                 128.4431|                8|           0| 53|               9|     0|\n",
      "+-------+-------------------------+-----------------+------------+---+----------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e88f9b",
   "metadata": {},
   "source": [
    "### There are outliers in the dataset that should be excluded before train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c60ad3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:================================================>    (182 + 13) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------+\n",
      "|summary|job_start_date|\n",
      "+-------+--------------+\n",
      "|  count|        500494|\n",
      "|   mean|          null|\n",
      "| stddev|          null|\n",
      "|    min|    1978-09-10|\n",
      "|    max|    3021-09-18|\n",
      "+-------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.describe(['job_start_date']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6213c289",
   "metadata": {},
   "source": [
    "## Save dataset for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e328b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_train.write.csv('train_model.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
